<!DOCTYPE html>
<html lang="zh">
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    <title>曹文强</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="A note is preferable to the best memory">
<meta property="og:type" content="website">
<meta property="og:title" content="曹文强">
<meta property="og:url" content="atlantic8.github.io/page/6/index.html">
<meta property="og:site_name" content="曹文强">
<meta property="og:description" content="A note is preferable to the best memory">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="曹文强">
<meta name="twitter:description" content="A note is preferable to the best memory">
    

    

    
        <link rel="icon" href="/css/images/logo.png">
    

    <link rel="stylesheet" href="/libs/font-awesome5/css/fontawesome.min.css">
    <link rel="stylesheet" href="/libs/font-awesome5/css/fa-brands.min.css">
    <link rel="stylesheet" href="/libs/font-awesome5/css/fa-solid.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    
        <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?ff86ad40748d96af89d192e9b0a3ae62";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    


</head>
</html>
<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">曹文强</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/.">Home</a>
                
                    <a class="main-nav-link" href="/archives">Archives</a>
                
                    <a class="main-nav-link" href="/categories/OJ">OJ</a>
                
                    <a class="main-nav-link" href="/categories/Algorithm">Algorithm</a>
                
                    <a class="main-nav-link" href="/categories/Math">Math</a>
                
                    <a class="main-nav-link" href="/categories/Dev">Dev</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/css/images/me.png" />
                            <i class="fas fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="想要查找什么..." />
            <span class="ins-close ins-selectable"><i class="fas fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/.">Home</a></td>
                
                    <td><a class="main-nav-link" href="/archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="/categories/OJ">OJ</a></td>
                
                    <td><a class="main-nav-link" href="/categories/Algorithm">Algorithm</a></td>
                
                    <td><a class="main-nav-link" href="/categories/Math">Math</a></td>
                
                    <td><a class="main-nav-link" href="/categories/Dev">Dev</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile" class="profile-fixed">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/css/images/me.png" />
            <h2 id="name">曹文强</h2>
            <h3 id="title">algorithm engineer</h3>
            <span id="location"><i class="fas fa-map-marker-alt" style="padding-right: 5px"></i>Beijing, China</span>
            <a id="follow" target="_blank" href="https://github.com/Atlantic8">关注我</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                132
                <span>文章</span>
            </div>
            <div class="article-info-block">
                48
                <span>标签</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="https://github.com/Atlantic8" target="_blank" title="github" class=tooltip>
                            
                                <i class="fab fa-github"></i>
                            
                        </a>
                    </td>
                    
                    <td>
                        <a href="https://www.linkedin.com/in/wenqiang-cao-704236b8/" target="_blank" title="linkedin" class=tooltip>
                            
                                <i class="fab fa-linkedin"></i>
                            
                        </a>
                    </td>
                    
                    <td>
                        <a href="mailto:atlantic8@outlook.com" target="_blank" title="envelope" class=tooltip>
                            
                                <i class="fas fa-envelope"></i>
                            
                        </a>
                    </td>
                    
                    <td>
                        <a href="http://weibo.com/2614093607" target="_blank" title="weibo" class=tooltip>
                            
                                <i class="fab fa-weibo"></i>
                            
                        </a>
                    </td>
                    
                    <td>
                        <a href="/" target="_blank" title="facebook" class=tooltip>
                            
                                <i class="fab fa-facebook"></i>
                            
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main">
    <article id="post-Markov-Decision-Process" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2017/01/06/Markov-Decision-Process/">Markov Decision Process</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2017/01/06/Markov-Decision-Process/">
            <time datetime="2017-01-06T02:07:53.000Z" itemprop="datePublished">2017-01-06</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/machine-learning/">machine learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p>马尔可夫决策过程MDP是经典的增强学习算法。MDP可以表示为一个元组<script type="math/tex">MDP=(S,A,\lbrace P_{sa} \rbrace,\gamma,R)</script>其中$S$是状态集合；$A$是动作集合，也就是可能的操作集合；$P_{sa}$为每一个状态$s$在每一个动作$a$上定义转移概率，转移到不同状态的概率不同；$\gamma \in [0,1]$为折扣因子；$R:S\times A \to \mathbb{R}$表示回报函数。</p>
<center>![MDP模型](http://ww2.sinaimg.cn/large/9bcfe727jw1fbgomcnv49j20b408wwf9.jpg)</center>

<hr>
<h5 id="贝尔曼方程（Bellman-equation）"><a href="#贝尔曼方程（Bellman-equation）" class="headerlink" title="贝尔曼方程（Bellman equation）"></a>贝尔曼方程（Bellman equation）</h5><p>贝尔曼方程是理查德贝尔曼提出的，它是典型的动态规划方程，也是动态规划最优性的必要条件。贝尔曼方程在最优控制理论中有着重要的作用。理查德贝尔曼证明了离散时间上的动态规划问题可以被表示成递归的、一步一步完成的后向推导形式，其中这过程需要写出价值函数的递推关系，其实贝尔曼方程的最基本思想就是<b>重叠子问题思想</b>。代价函数的递推关系被称为贝尔曼方程，其形式化表述如下。</p>
<p>假设时刻$t$时的状态为$s_t$，采取的动作是$a_t$，到达的新状态可以计算成$T(s_t,a_t)$，对应的收益为$F(s_t,a_t)$。联系折扣因子$\beta$，从$s_0$开始的无穷决策问题的收益是</p>
<script type="math/tex; mode=display">
\begin{aligned}
V(s_0)=\max_{\lbrace a_t \rbrace_{t=0}^{\infty}} \sum_{t=0}^{\infty} \beta^tF(s_t,a_t) \\
s.t.:\; s_{t+1}=T(s_t,a_t)
\end{aligned}</script><p>上是可以简化为</p>
<script type="math/tex; mode=display">
\begin{aligned}
V(s_0)=\max_{a_0} \lbrace F(s_0,a_0)+\beta V(s_1) \rbrace,\;s.t.:\;s_1=T(s_0,a_0)
\end{aligned}</script><hr>
<h5 id="MDP模型描述"><a href="#MDP模型描述" class="headerlink" title="MDP模型描述"></a>MDP模型描述</h5><hr>
<h6 id="MDP形式定义"><a href="#MDP形式定义" class="headerlink" title="MDP形式定义"></a>MDP形式定义</h6><p>MDP可表示成这样的过程</p>
<script type="math/tex; mode=display">
\begin{aligned}
s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1} s_2 \xrightarrow{a_2} s_3 \xrightarrow{a_3} ...
\end{aligned}</script><p>在上面的情况下，整体收益可以表示成</p>
<script type="math/tex; mode=display">
\begin{aligned}
R(s_0,a_0)+\gamma R(s_1,a_1) + \gamma^2 R(s_2,a_2) + ...
\end{aligned}</script><p>MDP的目标就是<b>选取一组动作以最大化收益均值</b>即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\arg \max_{a_0,a_1,...}E[R(s_0,a_0)+\gamma R(s_1,a_1) + \gamma^2 R(s_2,a_2) + ... ]
\end{aligned}</script><p>一个策略（policy）是由状态到动作的任意映射$\pi : S \to A$。执行一个策略$\pi$也就是对于任意状态$s$，系统采取的动作是$a=\pi(s)$。定义一个策略$\pi$的<b>价值函数（value function）</b>为</p>
<script type="math/tex; mode=display">
\begin{aligned}
V^{\pi}(s)&=E[R(s_0,a_0)+\gamma R(s_1,a_1) + \gamma^2 R(s_2,a_2) + ... |s_0=s,\pi] \\
V^{\pi}(s)&=R(s)+\gamma \sum_{s^{'}\in S}P_{s\pi(s)}(s^{'})V^{\pi}(s^{'})
\end{aligned}</script><p>表示从状态$s$开始，根据策略$\pi$执行的累积收益和的均值。<br>定义最优价值函数为</p>
<script type="math/tex; mode=display">
\begin{aligned}
V^{\*}(s)&=\max_{\pi} V^{\pi}(s) \\
V^{\*}(s)&=R(s)+\max_{a\in A}\gamma \sum_{s^{'}\in S}P_{sa}(s^{'})V^{\*}(s^{'})
\end{aligned}</script><p>指的是任何可能的策略下的最优解.</p>
<p>接下来定义<b>最优策略</b>，一个最优策略$\pi^{*}:S \to A$可以定义为</p>
<script type="math/tex; mode=display">
\begin{equation}
\pi^{\*}(s)=\arg\max_{a\in A}\sum_{s^{'}\in S}P_{sa}(s^{'})V^{\*}(s^{'})
\end{equation}</script><p>策略$\pi^{*}$为每一个状态提供了最优策略，也就是说无论初始状态是什么，$\pi^{*}$都是最优策略。事实上，对每一个状态$s$和每一个策略$\pi$，我们有</p>
<script type="math/tex; mode=display">
\begin{aligned}
V^{\*}(s)=V^{\pi^{\*}}(s) \ge V^{\pi}(s)
\end{aligned}</script><hr>
<h6 id="价值迭代和策略迭代"><a href="#价值迭代和策略迭代" class="headerlink" title="价值迭代和策略迭代"></a>价值迭代和策略迭代</h6><p>为了描述简单，这里仅考虑有限状态空间、有限动作空间的MDP。在下面两个算法中，转移概率$\lbrace P_{sa} \rbrace$和回报函数$R$都是已知的。</p>
<hr>
<p>价值迭代（value iteration）<br>1 初始化每个状态的价值$V(s)=0$<br>2 重复迭代直到收敛：<br>3 $\quad$对每个状态$s$，更新：$V(s)=R(s)+\max_{a\in A}\gamma \sum_{s^{‘}}P_{sa}(s^{‘})V(s{‘})$</p>
<hr>
<p>其中，更新方式有两种<br>同步更新：先计算所有状态的价值函数，然后再将其一起更新<br>异步更新：每计算出一个状态的价值函数，就将其更新<br>但无论是同步还是异步，$V$都会逐渐收敛至$V^{*}$。有了$V^{*}$，就可以根据公式$(1)$计算出对应的策略了。</p>
<hr>
<p>策略迭代（policy iteration）<br>1 随机初始化$\pi$<br>2 重复迭代直到收敛：<br>3 $\quad (a):$使得$V=V^{\pi}$<br>4 $\quad (b):$对每个状态$s$，使得$\pi(s)=\arg\max_{a\in A}\sum_{s^{‘}}P_{sa}(s^{‘})V(s^{‘})$</p>
<hr>
<p>迭代过程中，先计算在当前策略$\pi$下每个状态的价值函数值，然后根据这些值找到每个状态对应的最佳动作（greedy）。这些动作的集合就构成了下一步的新策略。步骤$(a)$的计算还是先赋值再迭代计算直到收敛，只是要按照$\pi$策略跳转而已。</p>
<hr>
<h6 id="学习一个MDP模型"><a href="#学习一个MDP模型" class="headerlink" title="学习一个MDP模型"></a>学习一个MDP模型</h6><p>上面的讨论中，转移概率$\lbrace P_{sa} \rbrace$和回报函数$R$都是已知的，但是实际中这两部分经常是未知的（通常$S,A,\gamma$是已知的），因此需要通过学习算法来学习。</p>
<p>数据可以是采集的，也可以是通过实验得出来的。对$P_{sa}$的估计方法可以是</p>
<script type="math/tex; mode=display">
\begin{aligned}
P_{sa}(s^{'})=\frac{times\; s \xrightarrow{a} s^{'}}{times\; s \xrightarrow{a} any\; state}
\end{aligned}</script><p>在样本不够大的情况下，可能出现$\frac{0}{0}$的情况，如果分母为0，则把对应的概率换成$\frac{1}{\vert S\vert}$.</p>
<p>$R$的更新思想类似。</p>
<hr>
<h5 id="连续状态的MDP"><a href="#连续状态的MDP" class="headerlink" title="连续状态的MDP"></a>连续状态的MDP</h5><p>现实中，MDP的离散状态假设是比较脆弱的。比如二维坐标中的位置就是连续的。那如何处理连续状态下的MDP呢</p>
<hr>
<h6 id="离散化"><a href="#离散化" class="headerlink" title="离散化"></a>离散化</h6><p>离散化的思想很直观，比如二维空间就可以通过网格化来达到离散化的目的。但是离散化有两个缺陷：<br><b>1 naive representation</b><br>对于$V^{*}$和$\pi^{*}$的表示太过简单，因为离散化会主动放弃潜在信息因此对平滑函数的表示效果比较差。比如离散化后的线性回归可能会得到如下图中的结果</p>
<center>![](http://ww3.sinaimg.cn/large/9bcfe727jw1fbfzpl9gu1j20cf09dq34.jpg)</center>

<p><b>2 维度诅咒</b><br>假设状态空间是$n$维的，离散化会使得离散化后的状态个数指数增加。</p>
<hr>
<h6 id="价值函数近似"><a href="#价值函数近似" class="headerlink" title="价值函数近似"></a>价值函数近似</h6><p>一般地，在连续MDP问题中通常假设</p>
<p>在价值迭代中，连续状态的迭代公式应该是</p>
<script type="math/tex; mode=display">
\begin{aligned}
V(s)&= R(s)+\gamma \max_{a} E_{s^{'}\sim P_{sa}}[V(s_{'}] \\
&= R(s)+\gamma \max_{a} \int_{s^{'}}P_{sa}(s^{'})V(s^{'})\,ds^{'}
\end{aligned}</script><p>此式跟上面的区别是把原来的求和改成了积分。</p>
<p>在对应状态为$s^{(1)},s^{(2)},…,s^{(m)}$的有限个数样本情况下，<b>价值函数近似就是要将价值函数近似为状态的函数</b>，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
V(s)=\theta^T\phi(s)
\end{aligned}</script><p>其中$\phi(s)$是状态$s$的合理映射。对于每一个样本$i$，算法先计算一个</p>
<script type="math/tex; mode=display">
\begin{aligned}
y^{(i)} \gets R(s)+\gamma \max_{a} E_{s^{'}\sim P_{sa}}[V(s_{'}]
\end{aligned}</script><p>上式的计算要使用采样逼近的原理，即采集多个样本求均值以逼近收集到每个样本的状态和对应的$y^{(i)}$，就可以使用监督学习算法训练出$V(s)$和$s$的模型。算法描述如下</p>
<hr>
<p>1 随机采样$m$个状态，$s^{(1)},s^{(2)},…,s^{(m)} \in S$.<br>2 初始化$\theta=0$.<br>3 迭代：<br>4 $\quad for\;i=1,…,m$.<br>5 $\quad \quad for\;each\;a\in A$.<br>5 $\quad \quad \quad $采样$s_1^{‘},…,s_k^{‘} \sim P_{s^{(i)}a}$.<br>6 $\quad \quad \quad $设置$q(a)=\frac{1}{k} \sum_{j=1}^k R(s^{(i)})+\gamma V(s_j^{‘})$.<br>7 $\quad \quad \quad $//因此$q(a)$就是$R(s^{(i)})+\gamma E_{s^{‘}\sim P_{s^{(i)}a}}[V(s_{‘}]$的估计.<br>8 $\quad \quad $令$y^{(i)}=\max_{a}q(a)$.<br>9 $\quad \quad $//因此$q(a)$就是$R(s^{(i)})+\gamma \max_{a} E_{s^{‘}\sim P_{s^{(i)}a}}[V(s_{‘}]$的估计.<br>10 $\quad$使得$\theta=\arg\min_{\theta}\frac{1}{2} \sum_{i=1}^m (\theta^T\phi(s^{(i)})-y^{(i)})^2$.</p>
<p>得到了近似于$V^{*}$的$V$，最后选择action时还是根据</p>
<script type="math/tex; mode=display">
\begin{aligned}
\arg\max_{a} E_{s^{'}\sim P_{sa}}[V(s^{'})]
\end{aligned}</script><hr>
<p>上述算法最后求$\theta$时使用的是线性回归方法，当然其他合适的方法也是可以的。<br>需要注意的是，价值函数近似方法并不能保证收敛，但是通常是收敛得。控制计算量的可用方法是调节算法第5步中的$k$值，有时候设置$k=1$也是可以的。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2017/01/06/Markov-Decision-Process/" data-id="ckg3volwv005tp0ph9lxge2ah" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Hidden-Markov-Model" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2016/12/31/Hidden-Markov-Model/">Hidden Markov Model</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2016/12/31/Hidden-Markov-Model/">
            <time datetime="2016-12-31T01:41:54.000Z" itemprop="datePublished">2016-12-31</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/machine-learning/">machine learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <h5 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h5><p>概率图模型是一类用图来表达变量相关关系的概率模型，常见的是用一个节点表示一个或一组随机变量，节点之间的边表示变量之间的概率关系。概率模型可以大致分为两类：</p>
<ul>
<li>第一类使用有向无环图表示变量之间的依赖关系，称之为有向图模型或者贝叶斯网</li>
<li>第二类使用无向无环图表示变量之间的依赖关系，称之为无向图模型或者马尔可夫网</li>
</ul>
<p>本文要介绍的隐马尔可夫模型就是结构简单的动态贝叶斯网。</p>
<h5 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h5><p>HMM的主要作用是时序数据建模，应用范围包括语音识别、自然语言处理等领域。<br>与马尔可夫过程不同，HMM中状态是无法直接观测的，取而代之，我们可以获取到与状态值息息相关的观测变量值，图中是经典的海藻与天气例子。</p>
<center>![经典的海藻于天气示例](http://ww2.sinaimg.cn/large/9bcfe727jw1fb9wwz6vk7j20eq08cdg2.jpg)</center>

<p>一个HMM模型中有两组变量，第一组是状态变量$Y=\lbrace y_1,y_2,…,y_n \rbrace$表示隐含的状态，下标表示时序。另一组是观测变量$X=\lbrace x_1,x_2,…,x_n \rbrace$，下标表示时序。系统可能会存在多个状态，这些状态的集合为$S=\lbrace s_1,s_2,…,s_N \rbrace$，如果$y_i = k$那么表示$i$时刻的状态为$s_k$。观测变量可以是离散的也可以是连续的，不妨假设其为离散值。同样地，定义观测值的集合$O = \lbrace o_1,o_2,…,o_M \rbrace$，如果$x_i = k$那么表示$i$时刻的状态为$o_k$。</p>
<p>HMM中有两个重要的性质：</p>
<ul>
<li>观测变量的取值仅依赖于状态变量</li>
<li>t时刻的状态仅依赖于t-1时刻的状态</li>
</ul>
<p>基于上面两个性质，可以得到如下公式</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(x_1,y_1,...,x_n,y_n)=p(y_1)p(x_1|y_1)\prod_{i=2}^n p(y_i|y_{i-1})p(x_i|y_i)
\end{aligned}</script><p>上式基本上描述了HMM的结构信息，在实际计算时还需要如下参数</p>
<hr>
<p>状态转移概率$A=[a_{ij}]_{N\times N}$，其中<script type="math/tex">a_{ij}=p(y_{t+1}=j|y_t=i)</script>表示由状态$s_i$转移到状态$s_j$的概率。</p>
<p>输出观测概率$B=[b_{ij}]_{N\times M}$，其中<script type="math/tex">b_{ij}=p(x_t=j|y_t=i)</script>表示在状态$s_i$下观测值为$o_j$的概率。</p>
<p>初始状态概率$\pi=(\pi_1,…,\pi_N)$，其中<script type="math/tex">\pi_i=p(y_1=i)</script>表示初始状态为$s_i$的概率。</p>
<hr>
<p>指定了状态空间$Y$，观测空间$X$和上述三组参数，一个HMM就确定了，所以一个HMM可以表示成一个五元组$(N,M,A,B,\pi)$，$N,M$分别表示状态值和观测值的可能取值范围。HMM也可以表示成$\lambda=(A,B,\pi)$。</p>
<p>下面考虑三个实际问题：</p>
<hr>
<h6 id="模型于观测序列匹配度"><a href="#模型于观测序列匹配度" class="headerlink" title="模型于观测序列匹配度"></a>模型于观测序列匹配度</h6><p>给定模型$\lambda=(A,B,\pi)$，如何有效计算其产生观测序列$X=(x_1,x_2,…,x_T)$的概率$p(X|\lambda)$？</p>
<p>为解决这一问题，Baum提出了<strong>前向算法</strong>，具体如下：<br><strong>定义$\theta_t(j)$为在$t$时刻，整体观测序列为$x_1,…,x_t$，此时状态为$s_j$的概率</strong>。其中我们有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_1(i) = \pi_ib_{ix_1}
\end{aligned}</script><p>表示第一个观测值的概率，其递推式也很容易写出</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1}(i)=\left[\sum_{j=1}^N\theta_t(j)a_{ji}\right]b_{ix_{t+1}}
\end{aligned}</script><p>有两部分构成，第一部分是由枚举$t$时刻的状态并跳转到$t+1$时刻，第二部分是$t+1$时刻的状态产生观测值的概率。</p>
<p>当$n=1$时，输出序列为$x_1$，此时计算概率$p(x_1|\lambda)$也就是计算初始状态集合每个可能的状态产生观测值$x_1$的概率和，也就是</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(x_1|\lambda)=\sum_{i=1}^N \theta_1(i)
\end{aligned}</script><p>当$n=2$时，输出序列为$x_1x_2$，所以有</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(x_1,x_2|\lambda) &=  \sum_{j=1}^N \theta_2(j)
\end{aligned}</script><p>后面的依此类推，前向算法的描述如下：</p>
<hr>
<p>1 初始化：$\theta_1(i)=\pi_ib_{i1}, 1\le i \le N$<br>2 $\theta_{t+1}(j)=\left[\sum_{i=1}^N \theta_t(i)a_{ij}\right]b_{jx_{t+1}}$<br>3 $p(x_1,…,x_T|\lambda)=\sum_{j=1}^N \theta_T(j)$</p>
<hr>
<p>一共有$T$个时刻，每个时刻要考虑$N$个状态，每个状态又要考虑钱一个时刻的$N$个状态，所以时间复杂度为$O(N^2T)$。</p>
<h6 id="推断隐藏序列"><a href="#推断隐藏序列" class="headerlink" title="推断隐藏序列"></a>推断隐藏序列</h6><p>由于有时候我们需要隐藏序列包含的信息，所以给出隐藏序列是有必要的（比如词性标注最终就需要给出词性序列）。问题的形式化表述即为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\arg\max_{y_1,...,y_T} p(y_1,...,y_T|x_1,...,x_T,\lambda)
\end{aligned}</script><p>这个问题的解决方法是维特比（Viterbi）算法。</p>
<p><strong>定义维特比变量$\gamma_t(j)$：表示在时序$t$，观察序列为$x_1,…,x_t$，状态为$s_j$的最大概率</strong>，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\gamma_t(j)=\max p(y_1,...,y_t=j|x_1,...,x_t,\lambda)
\end{aligned}</script><p>直观来说，在时序$t+1$状态为$s_j$的最大概率应该是时序$t$时所有状态转换到时序$t+1$、观测值为$x_{t+1}$并且状态为$s_j$的最大值，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\gamma_{t+1}(j)=\max_k \gamma_{t}(k)a_{kj}b_{jx_{t+1}}
\end{aligned}</script><p>并且，为了记忆路径，定义路径变量$\phi_t(j)$为该路径上的状态$s_j$的前一个状态，即从$t-1$时序到$t$的最优转移方式。</p>
<p>维特比算法的描述如下：</p>
<hr>
<p>1 初始化：<br>2 $\quad \gamma_1(j)=\pi_i b_{ix_{1}},\;\phi_1(i)=0,1\le i \le N$<br>3 归纳计算：<br>4 $\quad \gamma_t(j)=\max_k \gamma_{t-1}(k)a_{kj}b_{jx_{t}}$<br>5 $\quad \phi_t(i)=\arg\max_j \gamma_{t-1}(j)a_{jk}b_{kx_{t}}$<br>6 确定路径：<br>7 $\quad y_T=\arg\max_{y_j} \gamma_{T}(j)$<br>8 $\quad for\;t=T-1,…,1$<br>9 $\quad \quad y_t=\phi_{t+1}(y_{t+1})$</p>
<hr>
<h6 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h6><p>HMM的学习就是给定观测序列$X=x_1,…,x_T$，试图找到最优的参数$\lambda$使得$p(X|\lambda)$最大。如果知道了状态序列，那么$\pi,A,B$就都可以统计出来。但是状态序列其实是隐变量，求解此问题的方法是前向后向算法，也叫做Baum-Welch算法。</p>
<p>定义</p>
<script type="math/tex; mode=display">
\begin{aligned}
\beta_t(i)=p(x_{t+1},...,x_T|y_t=i,\lambda)
\end{aligned}</script><p>表示<strong>当前$t$时刻状态为$s_i$，部分观测序列为$x_{t+1},…,x_T$的概率</strong>。与前向算法类似，$\beta_t(i)$也可以有效地计算，公式如下</p>
<script type="math/tex; mode=display">
\begin{aligned}
\beta_t(i)&=\left[\sum_{j=1}^N \beta_{t+1}(j)a_{ij}\right]b_{jx_{t+1}},\; 1\le t \le T-1 \\
\beta_T(i)&=1
\end{aligned}</script><p>进一步可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(X,y_t=i|\lambda)&=p(x_1,...,x_t,y_t=i|\lambda)p(x_{t+1},...,x_T,y_t=i|\lambda)=\theta_t(i)\beta_t(i) \\
p(X,y_t=i|\lambda)&=\sum_{i=1}^N \theta_t(i)\beta_t(i)
\end{aligned}</script><center>![](http://ww2.sinaimg.cn/large/9bcfe727jw1fbacpj6v5zj20cc064jrl.jpg)</center>

<p>在前后向算法中，定义<script type="math/tex">\xi_t(i,j)=p(y_t=i,y_{t+1}=j|X,\lambda)</script>表示给定HMM和观测序列$X$，$t,t+1$时刻的状态分别是$s_i,s_j$的概率。如上图所示，对上式的推导为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\xi_t(i,j)&=\frac{p(y_t=i,y_{t+1}=j,X|\lambda)}{p(X|\lambda)} \\
&= \frac{\theta_t(i)a_{ij}\beta_{t+1}(j)b_{jx_{t+1}}} {\sum_{i=1}^N \sum_{j=1}^N \theta_t(i)a_{ij}\beta_{t+1}(j)b_{jx_{t+1}}}
\end{aligned}</script><p>除此之外，再定义<script type="math/tex">\eta_t(i)=p(y_t=i|X,\lambda)</script>表示给定HMM和观测序列$X$，$t$时刻状态为$s_i$的概率，所以有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\eta_t(i) = \frac{p(y_t=i,X|\lambda)} {p(X|\lambda)} = \frac{\theta_t(i)\beta_t(i)}{\sum_{i=1}^N \theta_t(i)\beta_t(i)}
\end{aligned}</script><p>考虑$\xi_t(i,j)$和$\eta_t(i)$的定义，就能够得到（想想就能得到）<script type="math/tex">\eta_t(i)=\sum_{i=1}^N \xi_t(i,j)</script>下面介绍前向后向算法的描述：</p>
<hr>
<p>1  初始化：随机初始化参数$A,B,\lambda$<br>2  不满足停止条件时迭代计算：<br>3  $\quad \beta_T(i)=1,1\le i \le N$<br>4  $\quad \beta_t(i)=\left[\sum_{j=1}^N \beta_{t+1}(j)a_{ij}\right]b_{jx_{t+1}},\; t \in \lbrace T-1,…,1 \rbrace,1\le i \le N $<br>5  $\quad \theta_{t}(i)=\left[\sum_{j=1}^N \theta_{t-1}(j)a_{ji}\right]b_{ix_{t}}$<br>6  $\quad$计算$\xi_t(i,j),\eta_t(i), 1\le i,j \le N,1 \le t \le T$<br>7  $\quad $更新参数：<br>8  $\quad \pi=\eta_1(i),1\le i \le N$<br>9  $\quad a_{ij}=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\eta_t(i)},1\le i,j \le N$<br>10 $\quad b_{jk}=\frac{\sum_{t=1,x_t=k}^T\quad \eta_t(j)} {\sum_{t=1}^{T}\eta_t(j)} ,1\le j \le N,1\le k \le M$<br>11 输出HMM：$\lambda=(A,B,\pi)$</p>
<p>$o_t=k$表示$t$时刻的观测值为第$k$种观测值。停止条件可以是：参数$\lambda=(A,B,\pi)$收敛。</p>
<hr>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2016/12/31/Hidden-Markov-Model/" data-id="ckg3volw20042p0phpggozg7b" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Support-Vector-Machine" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2016/12/27/Support-Vector-Machine/">Support Vector Machine</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2016/12/27/Support-Vector-Machine/">
            <time datetime="2016-12-27T02:59:50.000Z" itemprop="datePublished">2016-12-27</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/machine-learning/">machine learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <h5 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h5><hr>
<p>支持向量机是一种非常优秀的线性分类器。<br>给定数据集$D=\lbrace (x_1,y_1),(x_2,y_2),…,(x_m,y_m) \rbrace ,y_i\in \lbrace -1,1 \rbrace$，当$y_i=1$时，称$x_i$为正类；否则为负类。基本思想是在样本及空间内找到一个超平面将不同类别的样本分开。设超平面为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\omega^Tx+b=0
\end{aligned}</script><p>将特征空间划分两个部分，一部分是正类，一部分是负类，法向量指向的一侧为正类，反之为负类。相应的分类决策函数为$f(x)=sign(\omega^Tx+b)$。</p>
<hr>
<h6 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h6><p>定义超平面$(\omega,b)$关于样本点$(x_i,y_i)$的函数间隔为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{\gamma_i}=y_i(\omega^Tx_i+b)
\end{aligned}</script><p>函数间隔的正负可以表示分类结果是否准确，其大小可以<strong>相对地</strong>表示样本点到超平面的远近，越远置信度越高。<br>定义超平面$(\omega,b)$关于数据集$D$的函数间隔为$(\omega,b)$关于$D$中样本点$(x_i,y_i)$的函数间隔的最小值，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{\gamma}=\min_{i=1,...,m}\hat{\gamma_i}
\end{aligned}</script><p>函数间隔可以表示分类的准确度和置信度。但是，函数间隔还存在一个明显问题，比如将$\omega$和$b$都扩大2倍，超平面不变，但是函数间隔却扩大了2倍，解决方案是对函数间隔添加规范化约束。</p>
<h6 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h6><p>定义超平面$(\omega,b)$关于分类正确的样本点$(x_i,y_i)$的几何间隔为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\gamma_i=\frac{y_i( \omega^Tx_i+b )}{\Vert \omega \Vert}
\end{aligned}</script><p>定义超平面$(\omega,b)$关于数据集$D$的几何间隔为$(\omega,b)$关于$D$中样本点$(x_i,y_i)$的几何间隔的最小值，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\gamma=\min_{i=1,...,m}\gamma_i
\end{aligned}</script><h6 id="间隔最大化"><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a>间隔最大化</h6><p><strong>SVM学习的基本思想是求解能够正确划分训练数据集并且几何间隔最大的分离超平面</strong>，意味着以充分大的确信度对训练数据进行分类，以获得更好的泛化能力。所以问题就变成了</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \max_{\omega,b}\quad\gamma \\
& s.t. \quad \frac{y_i( \omega^Tx_i+b )}{\Vert \omega \Vert} \ge \gamma,\; i=1,2,...,m
\end{aligned}</script><p>考虑到$\gamma=\frac{\hat{\gamma} }{\Vert \omega \Vert}$，所以上式可以写成</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \max_{\omega,b}\quad\frac{\hat{\gamma} }{\Vert \omega \Vert} \\
& s.t. \quad y_i( \omega^Tx_i+b ) \ge \hat{\gamma},\; i=1,2,...,m
\end{aligned}</script><p>前面提到将$(\omega,b)$按比例缩放不会影响最终结果，所以可以取$\hat{\gamma}=1$，所以原问题可以转化为如下问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \max_{\omega,b}\quad\frac{1 }{\Vert \omega \Vert} \\
& s.t. \quad y_i( \omega^Tx_i+b ) \ge 1,\; i=1,2,...,m
\end{aligned}</script><p>即</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min_{\omega,b} \frac{1}{2}\Vert \omega \Vert^2 \\
& s.t. \quad y_i( \omega^Tx_i+b ) \ge 1,\; i=1,2,...,m
\end{aligned}</script><p>这就是SVM的基础形态，是一个凸二次规划问题。</p>
<hr>
<h6 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h6><p>考虑如下的优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{\omega} f(\omega) \\
\quad \quad s.t. \quad g_i(\omega) &\le 0  \\
h_i(\omega) &= 0
\end{aligned}</script><p>对应的拉格朗日函数如下，$\alpha,\beta$是拉格朗日乘子</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(\omega,\alpha,\beta)=f(\omega)+\sum_{i=1}^k\alpha_ig_i(\omega) + \sum_{i=1}^l\beta_ih_i(\omega).
\end{aligned}</script><p>考虑下面的优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_P(\omega)=\max_{\alpha,\beta:\alpha_i\ge 0} L(\omega,\alpha,\beta)
\end{aligned}</script><p>下标$P$是$primal$的意思，优化目标针对变量$\alpha,\beta$。给定$\omega$，如果$\omega$的任何一个分量违背了限制条件(比如$g_i(\omega)&gt;0$或者$h_i(\omega) \neq 0$)，那么$\theta_P(\omega)$就是无穷大！反之，如果限制条件没有被打破，那么$\theta_P(\omega)=f(\omega)$，所以</p>
<script type="math/tex; mode=display">
\theta_P(\omega)=\begin{cases}
f(\omega)   &    如果 \omega满足限制条件 \\
\infty      &    其他
\end{cases}</script><p>下式成立</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{\omega} \theta_P(\omega) = \min_{\omega} \max_{\alpha,\beta:\alpha_i\ge 0} L(\omega,\alpha,\beta) = p^{\*}
\end{aligned}</script><hr>
<p>下面考虑对偶问题，定义</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_D(\alpha,\beta) = \min_{\omega} L(\omega,\alpha,\beta)
\end{aligned}</script><p>这里下标$D$表示$dual$，优化目标针对变量$\omega$。所以有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max_{\alpha,\beta:\alpha_i\ge 0} \theta_D(\alpha,\beta) = \max_{\alpha,\beta:\alpha_i\ge 0} \min_{\omega} L(\omega,\alpha,\beta) = d^{\*}
\end{aligned}</script><hr>
<p>对于上面的$p^{*}$和$d^{*}$，有如下的规律</p>
<script type="math/tex; mode=display">
\begin{aligned}
d^{\*}=\max_{\alpha,\beta:\alpha_i\ge 0} \min_{\omega} L(\omega,\alpha,\beta) \le \min_{\omega} \max_{\alpha,\beta:\alpha_i\ge 0} L(\omega,\alpha,\beta) = p^{\*}
\end{aligned}</script><p>上式中等号成立的条件是：函数$f$和$g_i$都是凸函数，$h_1,h_2,…,h_l$函数是同族函数。并且每个$g_i$函数都是严格合理的，即对每一个$i$，都存在一些$\omega$满足$g_i(\omega)小于0$。此外，存在$(\alpha,\beta,\omega)$满足<strong>KKT条件</strong>，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{dL(\alpha,\beta,\omega)}{d\omega_i} &= 0,\quad i=1,...,m \\
h_i(\omega)&= 0,\quad i=1,...,l \\
\alpha_i g_i(\omega) &= 0, \quad i=1,...,k \\
g_i(\omega) &\le 0, \quad i=1,...,k \\
\alpha_i &\ge 0 , \quad i=1,...,k
\end{aligned}</script><hr>
<h6 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h6><p>由最大化间隔得出的优化问题是一个凸二次规划问题，可以使用现成的工具完成。使用拉格朗日乘子法可得到其对偶问题，为其每项约束添加一个拉格朗日乘子$\alpha_i \ge 0$，拉格朗日目标函数为</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(\omega,b,\alpha) = \frac{1}{2}\Vert \omega \Vert^2 + \sum_{i=1}^m \alpha_i(1-y_i(\omega^Tx_i+b))
\end{aligned}</script><p>其中，$\alpha=(\alpha_1;\alpha_2;…;\alpha_m)$。上式分别对$\omega,b$求偏导并令其为0可得</p>
<script type="math/tex; mode=display">
\begin{aligned}
\omega = \sum_{i=1}^m \alpha_iy_ix_i \\
0 = \sum_{i=1}^m \alpha_iy_i
\end{aligned}</script><p>将上式代入$L(\omega,b,\alpha)$中，消去$\omega,b$，则上式变成</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(\omega,b,\alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i^T x_j
\end{aligned}</script><p>由于满足KKT条件，所以原问题$f(\omega)$的优化问题可以写成</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{\omega,b} f(\omega) &\to \min_{\omega,b} \max_{\alpha} L(\omega,b,\alpha) \\
&\to \max_{\alpha} \min_{\omega,b} L(\omega,b,\alpha)
\end{aligned}</script><p>这里$ \max_{\alpha} L(\omega,b,\alpha)$的意义在于选择参数$\alpha$使得$ L(\omega,b,\alpha)$最大，根据$ L(\omega,b,\alpha)$的公式，由于$1\le y_i(\omega^Tx_i+b)$，所以一旦出现$1 &lt; y_i(\omega^Tx_i+b)$的情况，就应该有$\alpha_i=0$，否则这一项就是负值，整体也就不是最小值了。所以，优化问题最终转化为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max_{\alpha} \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i^T x_j \\
s.t. \; \sum_{i=1}^m\alpha_iy_i=0,\alpha_i \ge 0,\; i=1,...,m
\end{aligned}</script><hr>
<h6 id="Sequential-Minimal-Optimization"><a href="#Sequential-Minimal-Optimization" class="headerlink" title="Sequential Minimal Optimization"></a>Sequential Minimal Optimization</h6><p>对偶问题依旧是二次规划问题，但问题的规模正比于训练样本数。SMO是高效算法之一，其思想是每次选取两个变量$\alpha_i,\alpha_j$并固定其他的$\alpha_k$，不断执行迭代步骤。这种方法也称为坐标上升方法（coordinate ascent）。</p>
<p>令<script type="math/tex">\alpha_iy_i+\alpha_jy_j=-\sum_{k\neq i,j}\alpha_ky_k=c</script>用这个式子消去优化目标函数中的$\alpha_j$可以得到一个以$\alpha_i$为单变量的二次规划问题，该问题有闭式解且简单易懂。解出之后，$\alpha_i,\alpha_j$就得到了更新。SMO选取$\alpha_i,\alpha_j$时，启发式地选择对应样本间隔最大的变量进行更新。</p>
<p>然后根据$\omega = \sum_{i=1}^m \alpha_iy_ix_i$求出$\omega$，对于$b$，可以使用任意一个支持向量的性质$y_s \left(\omega^Tx_s+b \right)=1$来计算。当然更鲁棒的方法是使用所有的支持向量并对求出的$b$取均值。</p>
<hr>
<h5 id="软间隔支持向量机（Soft-Margin-SVM）"><a href="#软间隔支持向量机（Soft-Margin-SVM）" class="headerlink" title="软间隔支持向量机（Soft Margin SVM）"></a>软间隔支持向量机（Soft Margin SVM）</h5><p>基础型的SVM的假设所有样本在样本空间是线性可分的(硬间隔)，但是现实中的情况通常不满足这种特性。对应的软间隔允许某些样本不满足</p>
<script type="math/tex; mode=display">
\begin{aligned}
y_i(\omega^Tx_i+b)\ge 1
\end{aligned}</script><p>为了解决这个问题，可以对每个样本点引入一个松弛变量$\xi_i\ge 0$满足</p>
<script type="math/tex; mode=display">
\begin{aligned}
y_i(\omega^Tx_i+b)+\xi_i\ge 1
\end{aligned}</script><p>同时，对每个松弛变量支付一个代价$\xi_i$。训练时应该要保持不满足约束的样本尽量少，所以优化函数可以表示成</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min_{\omega,b,\xi_i} \frac{1}{2}\Vert \omega \Vert^2 + C\sum_{i=1}^m \xi_i \\
&s.t. \; y_i(\omega^Tx_i+b) \ge 1-\xi_i \\
&\quad \xi_i \ge 0,\;i=1,...,m
\end{aligned}</script><p>其中，$C&gt;0$是一个惩罚参数，调节损失函数两项的权重。<br>这就是常用的软间隔支持向量机，通过拉格朗日乘子法，得到如下拉格朗日函数</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(\omega,b,\alpha,\xi,\mu) = \frac{1}{2}\Vert \omega \Vert^2 + C\sum_{i=1}^m \xi_i + \sum_{i=1}^m \alpha_i(1-\xi_i-y_i(\omega^Tx_i+b)) - \sum_{i=1}^m \mu_i\xi_i
\end{aligned}</script><p>其中，$\alpha_i,\mu_i$就是拉格朗日乘子。令$L(\omega,b,\alpha,\xi,\mu)$对$\omega,b,\xi$的偏导数为0得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
\omega &= \sum_{i=1}^m \alpha_iy_ix_i \\
0 &= \sum_{i=1}^m \alpha_iy_i \\
C &= \alpha_i + \mu_i
\end{aligned}</script><p>由上式可知$0 \le \alpha_i \le C$。将上式代入$L(\omega,b,\alpha,\xi,\mu)$中，并求解优化函数的对偶形式</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\max_{\alpha} \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i^T x_j \\
&s.t. \sum_{i=1}^m\alpha_iy_i=0 \\
&0 \le \alpha_i \le C,\; i=1,...,m
\end{aligned}</script><p>对于软间隔支持向量机，其KKT条件是</p>
<script type="math/tex; mode=display">
\begin{aligned}
\alpha_i y_i(\omega^x_i+b)-1+\xi_i &= 0, \quad i=1,...,m \\
y_i(\omega^x_i+b)-1+\xi_i &\le 0, \quad i=1,...,m \\
\alpha \ge 0,\mu_i &\ge 0, \quad i=1,...,m \\
\xi_i \ge 0, \mu_i \xi_i &= 0, \quad i=1,...,m
\end{aligned}</script><p>对于非支持向量，$\alpha_i=0$。对支持向量，有$0 &lt; \alpha_i \le C$，即有$y_i(\omega^x_i+b)=1-\xi_i$，样本点$x_i$到间隔边界的距离为$\frac{\xi_i}{\Vert \omega \Vert}$，软间隔的支持向量有以下几种情况</p>
<ul>
<li>或者在间隔边界上（$\alpha_i &lt; C,\xi_i=0$）</li>
<li>或者在间隔边界与分离超平面之间（$\alpha_i=C,0 &lt; \xi_i &lt; 1$，此时分类也是正确的）</li>
<li>或者在分离超平面误分那一侧（$\alpha_i=C,\xi_i&gt;1$，分类错误，该样本是异常点）</li>
</ul>
<p>上述优化问题依旧可以使用SMO算法，求出$\omega$之后，$b$的求法如下。注意到满足$0&lt;\alpha_i &lt; C$的样本是支持向量，即满足$y_i(\omega^Tx_i+b)=1$，所以$b$就可解了。</p>
<h5 id="非线性核支持向量机（Kernal-SVM）"><a href="#非线性核支持向量机（Kernal-SVM）" class="headerlink" title="非线性核支持向量机（Kernal SVM）"></a>非线性核支持向量机（Kernal SVM）</h5><h6 id="核函数判定定理"><a href="#核函数判定定理" class="headerlink" title="核函数判定定理"></a>核函数判定定理</h6><p>设$X$是输入空间，$k(\cdot,\cdot)$是定义在$X\times X$的对称函数，则$k$是核函数当且仅当对于任意数据集$D=\lbrace x_1,x_2,…,x_m \rbrace$，核矩阵$K$总是半正定的，其中半正定是指对于每个非零的复向量$z$，$z^{*}Kz &gt; 0$，其中$z^{*}$是$z$的共轭转置。</p>
<p>常用的核函数有</p>
<script type="math/tex; mode=display">
\begin{aligned}
Linear\; kernal &: \quad k(x_i,x_j)= x_i^Tx_j \\
Multinomial\; kernal &: \quad k(x_i,x_j)= (x_i^Tx_j)^d \\
Gaussian\; kernal &: \quad k(x_i,x_j)= e^{-\frac{\Vert x_i-x_j \Vert^2}{2\sigma^2}} \\
Laplace\; kernal &: \quad k(x_i,x_j)= e^{-\frac{\Vert x_i-x_j \Vert}{\sigma}} \\
Sigmoid\; kernal &: \quad k(x_i,x_j)= tanh(\beta x_i^Tx_j+\theta),\beta>0,0>\theta
\end{aligned}</script><p>另外核函数的线性组合、乘积也是核函数。并且对于任意函数$g(\cdot)$，$k(x,z)=g(x)k_1(x,z)g(z)$也是核函数。</p>
<hr>
<h6 id="KSVM"><a href="#KSVM" class="headerlink" title="KSVM"></a>KSVM</h6><p>基础型的SVM的假设样本在样本空间是线性可分的，但是现实中的情况通常不满足这种特性。对于这种问题，一种可能的方法是将样本从原始空间映射到更高维的特征空间，使得其在线性可分。令$\phi (x)$表示将$x$映射后的特征向量，于是特征空间中的超平面可以表示为<script type="math/tex">f(x)=\omega^T \phi (x)+b</script>优化的对偶问题变成</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max_{\alpha} \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \phi (x_i)^T \phi (x_j) \\
s.t. \; \sum_{i=1}^m\alpha_iy_i=0,\alpha_i \ge 0,\; i=1,...,m
\end{aligned}</script><p>由于可能存在维度诅咒，计算$\phi (x_i)^T \phi (x_j)$将非常困难。这里引入<strong>核函数</strong>的概念</p>
<script type="math/tex; mode=display">
\begin{aligned}
k(x_i,x_j) = \langle \phi (x_i),\phi (x_j) \rangle = \phi (x_i)^T \phi (x_j)
\end{aligned}</script><p>满足$x_i,x_j$在特征空间的内积等于它们在原始样本空间中通过函数$k(\cdot)$计算的结果，核函数是避免维度诅咒的一种方法。</p>
<p>引用<br>[1]. 统计学习方法. 李航著. 清华大学出版社<br>[2]. 机器学习. 周志华.</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2016/12/27/Support-Vector-Machine/" data-id="ckg3vom25009xp0phtse4xpli" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Sparse-Coding" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2016/12/20/Sparse-Coding/">Sparse Coding</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2016/12/20/Sparse-Coding/">
            <time datetime="2016-12-20T02:26:20.000Z" itemprop="datePublished">2016-12-20</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/machine-learning/">machine learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <h5 id="字典学习"><a href="#字典学习" class="headerlink" title="字典学习"></a>字典学习</h5><p>字典学习是与稀疏性相关的学习方法，它被用来寻找一组“超完备”基向量来更高效地表示样本数据。稀疏性会降低计算和存储的开销，并且会提高模型的可解释性。</p>
<pre><code>稀疏表示侧重于学习一个字典
</code></pre><p>给定数据集$\lbrace x_1,x_2,…,x_m \rbrace$，字典学习的简单形式如下</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{B,\alpha_i} \sum_{i=1}^m \Vert x_i-B\alpha_i \Vert_2^2 + \lambda \sum_{i=1}^m \Vert \alpha_i \Vert_1
\end{aligned}</script><p>其中$B\in R^{d\times k}$为字典矩阵，$k$称为字典的词汇量，$\alpha_i \in R^k$是样本$x_i$的稀疏表示。实际中，根据应用场景的要求，第二项中的$\lambda \sum_{i=1}^m \Vert \alpha_i \Vert_1$也可以替换成别的代价函数。</p>
<h6 id="奇异值分解简述SVD"><a href="#奇异值分解简述SVD" class="headerlink" title="奇异值分解简述SVD"></a>奇异值分解简述SVD</h6><p>对任意矩阵奇异值分解<script type="math/tex">M=U\Sigma V^T</script>其中，$\Sigma$是对角矩阵，对角线上是矩阵的奇异值，$U$中的每一列是经过$M$转化后的标准正交基组成之一，而$V$表示了原始域的标准正交基，每一列是一个向量。</p>
<blockquote>
<p>以后应该还有关于SVD的解释</p>
</blockquote>
<h6 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h6><p>上式的训练目标参数有$\alpha_i,B$，可以用<b>交替优化的方法</b>。</p>
<hr>
<p>1 为每个样本$x_i$找到相应的$\alpha_i$，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{\alpha_i} \Vert x_i-B\alpha_i \Vert_2^2 + \lambda \Vert \alpha_i \Vert_1
\end{aligned}</script><p>这一步可以使用lasso的优化方法。</p>
<hr>
<p>2 以$\alpha_i$为初值来更新字典$B$，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{B} \Vert X-BA \Vert_F^2
\end{aligned}</script><p>其中，$X=(x_1,x_2,…,x_m)\in R^{d\times m},A=(\alpha_1,\alpha_2,…,\alpha_m)\in R^{k\times m}$，矩阵的F范数是矩阵每个元素的平方和再开方。</p>
<p>上式的常用优化方法为<b>KSVD</b>，这是基于逐列更新的方法。令$b_i$表示字典$B$的第$i$列，$\alpha^i$表示稀疏矩阵$A$的第$i$行($\alpha_i$表示稀疏矩阵$A$的第$i$列)，$b_i\alpha^i$其实是个矩阵。则上式可以重写成</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{B} \Vert X-BA \Vert_F^2 &= \min_{b_i} \left\Vert X-\sum_{j=1}^k b_ja^j \right\Vert_F^2 \\
&= \min_{b_i} \left\Vert X-\sum_{j=1,j\not i}^k b_ja^j - b_i\alpha^i \right\Vert_F^2 \\
&= \min_{b_i} \Vert E_i-b_i\alpha^i \Vert_F^2
\end{aligned}</script><p>更新字典第$i$列时，$E_i$是固定的，$E_i$表示没有$b_i$时表示的误差。所以最小化上式原则上就是对$E_i$进行奇异值分解(SVD)取得最大奇异值所对应的正交向量。对$E_i$进行奇异值分解$E_i=U\Sigma V^T$，那么$b_i$是$U$中最大奇异值对应的正交向量，但此时$\alpha^i$也需要更新，这就会破坏$\alpha^i$的稀疏性质。</p>
<p>为了避免这种情况，KSVD做如下处理。注意到要保持稀疏性，我们不能将$\alpha_i$原来为0的位置变成非零数。$b_i \alpha^i$是一个矩阵，它的第$j$列是$b_i\alpha_j^i$。如果原来$\alpha_j^i=0$，那么$b_i\alpha_j^i$就是全为0的列向量。这里，我们可以把$E_i$对应位置的列变成全0，这样奇异值分解后的结果就不会破坏$A$的稀疏性。然后，对$E_i$进行奇异值分解，更新$b_i$为$U$中最大奇异值对应的正交向量，更新$\alpha^i$为$V$中最大奇异值对应的正交向量(列向量)乘以最大奇异值。</p>
<hr>
<h5 id="稀疏编码"><a href="#稀疏编码" class="headerlink" title="稀疏编码"></a>稀疏编码</h5><p>稀疏编码是字典学习的下一步，在学习到了字典$B$后，给定一个新的样本$x_k$，只需要通过优化</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{\alpha_i}  \Vert x_k-B\alpha_k \Vert_2^2 + \lambda \Vert \alpha_k \Vert_1
\end{aligned}</script><p>来找到$\alpha_k$，即求解出了基于字典$B$的关于$x_k$的稀疏表示。上式的优化方法即经典的Lasso优化问题，使用Proximal Gradient Descent方法。</p>
<p>比如说：</p>
<center>![](http://img.my.csdn.net/uploads/201304/09/1365483491_9524.jpg)</center>
        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2016/12/20/Sparse-Coding/" data-id="ckg3vom1k009cp0phs21zu0e6" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Manifold-Learning" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2016/12/18/Manifold-Learning/">Manifold Learning</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2016/12/18/Manifold-Learning/">
            <time datetime="2016-12-18T03:50:16.000Z" itemprop="datePublished">2016-12-18</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/machine-learning/">machine learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <h6 id="流形"><a href="#流形" class="headerlink" title="流形"></a>流形</h6><pre><code>流形学习是一类借鉴了拓扑流行概念的概念降维方法
直观上来讲，一个流形好比是一个 d 维的空间
在一个 m 维的空间中 (m &gt; d) 被扭曲之后的结果
</code></pre><p>比如说一块布，可以把它看成一个二维平面，这是一个二维的欧氏空间，现在我们（在三维）中把它扭一扭，它就变成了一个流形（当然，不扭的时候，它也是一个流形，欧氏空间是流形的一种特殊情况），地球表面其实也只是一个二维流形。</p>
<pre><code>流形的一个特点是：流形是在局部与欧式空间 同胚 的空间
即局部上具有欧式空间的特性，距离度量可以使用欧氏距离
</code></pre><p>所以，低维流形嵌入到高维空间中，数据样本在高维空间中的分布看上去会比较复杂，但在局部上具有欧式空间的特性。因此，可以容易地在局部建立降维映射关系，然后设法将局部关系映射到全局。<b>此种降维方法可被用于数据可视化</b>。</p>
<hr>
<h6 id="等度量映射（Isometric-Mapping）"><a href="#等度量映射（Isometric-Mapping）" class="headerlink" title="等度量映射（Isometric Mapping）"></a>等度量映射（Isometric Mapping）</h6><p>Isomap的出发点是：低维嵌入流形上两点距离是“测地线距离”（地理上的概念，比如地球上两点的距离就不是欧氏距离）<br>但是利用流形的局部与欧式空间同胚特性，我们就能<b>在局部空间内找到每个点的近邻点，从而建立一个近邻连接图。<br>所以，“测地线距离”就是图中的最短距离</b>。最短路径算法可以使用Dijkstra算法或者Floyd算法。<br>有了距离度量表示，就可以降维了，降维方法可以使用MDS算法（当然也可以使用其他方法）</p>
<p>Isomap算法描述如下：</p>
<hr>
<p>输入：样本集$D= \lbrace x_1,x_2,…,x_m \rbrace $; 近邻参数$k$; 低维空间维度$d^{‘}$</p>
<hr>
<p>1 $for\quad i=1,…,m \quad do$<br>2 $\quad$确定$x_i$的$k$近邻<br>3 $\quad x_i$与其$k$近邻的距离设置为其欧氏距离，与其他点的距离设置为无穷大<br>4 $end \quad for$<br>5 调用最短距离路径算法计算任意两点之间的距离$dist(x_i,x_j)$<br>6 将$dist(x_i,x_j)$作为MDS算法的输入，输出结果$Z$</p>
<hr>
<p>输出：样本集$D$在低维空间的投影$Z$</p>
<hr>
<h6 id="局部线性嵌入（Locally-Linear-Embedding）"><a href="#局部线性嵌入（Locally-Linear-Embedding）" class="headerlink" title="局部线性嵌入（Locally Linear Embedding）"></a>局部线性嵌入（Locally Linear Embedding）</h6><p>Isomap 希望保持任意两点之间的测地线距离，保存的信息量大，但是计算量随着节点数量的增长爆炸增长（Dijkstra算法$O(n^2)$或者Floyd算法$O(n^3)$）。<br>LLE 希望保持局部线性关系，信息量较小，但是对数据量较大的情形则比较有效。</p>
<p>LLE假设$x_i$能够通过其邻域内的样本$x_j,x_k,x_l$线性表出，即<script type="math/tex">x_i=\omega_{ij}x_j+\omega_{ik}x_k+\omega_{il}x_l</script>所以，LLE需要先找出每个样本$x_i$的近邻下标集合$Q_i$，然后计算出基于$Q_i$中的样本对$x_i$进行线性重构的系数$\omega_i$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{\omega_1,\omega_2,...,\omega_m} \sum_{i=1}^m \left\Vert x_i-\sum_{j \in Q_i}\omega_{ij}x_j \right\Vert^2 \\
s.t. \quad \sum_{j \in Q_i}\omega_{ij}=1
\end{aligned}</script><p>这里令$C_{jk}=(x_i-x_j)^T(x_i-x_k)$，$\omega_{ij}$有闭式解</p>
<script type="math/tex; mode=display">
\begin{aligned}
\omega_{ij}=\frac{\sum_{k\in Q_i}C_{jk}^{-1}}{\sum_{l,s\in Q_i}C_{ls}^{-1}}
\end{aligned}</script><p>因为LLE假设在低维空间中保持$\omega_i$保持不变，令$Z=(z_1,z_2,…,z_m)\in R^{d^{‘}\times m},W_{ij}=\omega_{ij}$.所以$x_i$的低维坐标$z_i$可以通过</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{z_1,z_2,...,z_m} \sum_{i=1}^m \left\Vert z_i-\sum_{j\in Q_i}\omega_{ij}z_j \right\Vert^2 \\
\min_{z_1,z_2,...,z_m} \sum_{i=1}^m \left\Vert z_i-W_iZ \right\Vert^2
\end{aligned}</script><p>求解，这里需要对$z_i$正规化以满足$\sum_i z_i=0,\frac{1}{m}\sum_i z_iz_i^T=I$。可以令$M=(I-W)^T(I-W)$，那么优化函数可以写成<script type="math/tex">\min_Z tr(ZMZ^T) \quad s.t \; ZZ^T=I</script>这里需要注意我们假设对$Z$正规化，才能满足$ZZ^T=I$。然后上面的优化函数可以通过特征值分解，取最小的$d^{‘}$个非零特征值对应的特征向量即为$Z^T$。</p>
<p>LLE算法描述</p>
<hr>
<p>输入：样本集$D={x_1,x_2,…,x_m}$; 近邻参数$k$; 低维空间维度$d^{‘}$</p>
<hr>
<p>1 $for\; i=1,…,m \quad do$<br>2 $\quad$确定$x_i$的$k$近邻<br>3 $\quad$求$\omega_{ij},\; j\in Q_i$, 不在$x_i$邻域内的系数为0<br>4 $end \; for$<br>5 求矩阵$M$<br>5 对$M$进行特征值分解<br>6 输出最小的$d^{‘}$个非零特征值对应的特征向量</p>
<hr>
<p>输出：样本集$D$在低维空间的投影$Z$</p>
<hr>
<h6 id="拉普拉斯特征映射-Laplacian-Eigenmaps"><a href="#拉普拉斯特征映射-Laplacian-Eigenmaps" class="headerlink" title="拉普拉斯特征映射(Laplacian Eigenmaps)"></a>拉普拉斯特征映射(Laplacian Eigenmaps)</h6><blockquote>
<p>待更……</p>
</blockquote>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2016/12/18/Manifold-Learning/" data-id="ckg3volwv005pp0ph0037kb88" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Multiple-Dimensional-Scaling" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2016/12/18/Multiple-Dimensional-Scaling/">Multiple Dimensional Scaling</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2016/12/18/Multiple-Dimensional-Scaling/">
            <time datetime="2016-12-18T02:22:41.000Z" itemprop="datePublished">2016-12-18</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/machine-learning/">machine learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p>Multiple Dimensional Scaling (MDS、多维缩放)是经典的聚类算法。</p>
<hr>
<p>假设原始$d$维样本空间中有$m$个样本$x_1,x_2,…,x_m$，其距离矩阵为$D \in R^{m\times m}$，$dist_{ij}$为$x_i$到$x_j$的距离。MDS的出发点是获得样本在$d^{‘}$维空间内的表示$Z\in R^{d^{‘} \times m}$，新空间内样本的距离等于原始空间的距离，即<script type="math/tex">\Vert z_i-z_j \Vert = dist_{ij}</script>令$B=Z^TZ \in R^{m\times m}$，其中$B$为降维后的样本内积矩阵，$b_{ij}=z_i^Tz_j$，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
dist_{ij}^2 &= \Vert z_i \Vert + \Vert z_j \Vert - 2z_i^Tz_j \\
&= b_{ii}+b_{jj}-2b_{ij}
\end{aligned}</script><p>假设样本$Z$被中心化(normalization)，即$\sum_{i=1}^m z_i=0$。于是有<script type="math/tex">\sum_{i=1}^mb_{ij}=\sum_{j=1}^mb_{ij}=0</script>。继而有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i=1}^m dist_{ij}^2 = tr(B)+mb_{jj} \\
\sum_{j=1}^m dist_{ij}^2 = tr(B)+mb_{ii} \\
\sum_{i=1}^m \sum_{j=1}^m dist_{ij}^2 = 2mtr(B)
\end{aligned}</script><p>其中，$tr$表示矩阵的秩，$tr(B)=\sum_{i=1}^m \Vert z_i \Vert^2$，联立上式可得<script type="math/tex">b_{ij}=-\frac{1}{2} \left( dist_{ij}^2 - \frac{1}{m}\sum_{i=1}^m dist_{ij}^2 -\frac{1}{m}\sum_{j=1}^m dist_{ij}^2 + \frac{1}{m^2}\sum_{i=1}^m \sum_{j=1}^m dist_{ij}^2 \right)</script>由此即可通过降维前后保持不变的距离矩阵$D$求取内积矩阵$B$。</p>
<p>那么，下面就需要求$Z$了。方法是对$B$做特征值分解，得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
B=Z^TZ=V\Lambda V^T
\end{aligned}</script><p>其中，$\Lambda=diag(\lambda_1,…,\lambda_{d})$为$d$个特征值构成的对角矩阵，$\lambda_1 \ge \lambda_2 \ge … \ge \lambda_d$。令$\Lambda_{*}=diag(\lambda_1,…,\lambda_{d^{*}})$为对角矩阵，$\Lambda_{*}$表示对应的特征向量矩阵，则<script type="math/tex">Z=\Lambda_{\*}^{\frac{1}{2}}V_{\*}^T \in R^{d^{\*}\times m}</script>现实中，仅需要降维后的距离与原始空间内的距离尽可能接近，不必严格相等。此时可取$d^{*}$远小于$d$求解。</p>
<p>MDS算法描述如下：</p>
<hr>
<p>输入：距离矩阵$D\in R^{m\times m}$，其元素$dist_{ij}$为样本$x_i$到$x_j$的距离；低维空间维度$d^{‘}$</p>
<hr>
<p>1 计算矩阵$B$<br>2 对$B$做特征值分解<br>3 取$\Lambda$为$d^{‘}$个最大特征值所构成的对角矩阵，$V$为相应的特征向量矩阵</p>
<hr>
<p>输出：$Z=\Lambda^{\frac{1}{2}}V^T$，其中$Z$的每列对应一个样本的低维坐标</p>
<hr>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2016/12/18/Multiple-Dimensional-Scaling/" data-id="ckg3volxn006vp0phe9lj4p06" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Logistic-Regression" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2016/12/13/Logistic-Regression/">Logistic Regression</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2016/12/13/Logistic-Regression/">
            <time datetime="2016-12-13T05:52:06.000Z" itemprop="datePublished">2016-12-13</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/machine-learning/">machine learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <h5 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h5><p>Logistic Regression的思想源于<b>广义线性模型</b><script type="math/tex">y=g(\omega^Tx+b)</script>其中，函数$g()$称为联系函数。<br>逻辑回归中，联系函数其实<b>sigmoid函数</b>: <script type="math/tex">y=\frac{1}{1+e^{-z}}</script>其函数图如下：</p>
<center>![](http://ww2.sinaimg.cn/large/9bcfe727jw1fap4kz80uzj208w05xt8q.jpg)</center>

<p>令$z=\omega^T x $，所以<script type="math/tex">y=\frac{1}{1+e^{-(\omega^T x)}}</script>其中，（通过扩展训练数据将bias部分加入，就省去了bias部分）上式可以写成<script type="math/tex">ln\frac{y}{1-y}=\omega^T x</script>如果把$y$当作正样本的后验概率$p(y=1|x)$，那么$1-y$就是负样本的后验概率$p(y=0|x)$。所以有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y=1|x) = \frac{e^{-(\omega^T x)}}{1+e^{-(\omega^T x)}} \\
p(y=0|x) = \frac{1}{1+e^{-(\omega^T x)}}
\end{aligned}</script><p>我们通过极大似然法来估计$\omega$，给定数据集${(x_i,y_i)}_{i=1}^m$，对数似然函数可以写成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
l(\omega) &= ln\prod_{i=1}^m \left(\frac{e^{-\omega^T x_i}}{1+e^{-\omega^T x_i}}\right)^{y_i} \cdot \left(\frac{1}{1+e^{-\omega^T x_i}}\right)^{1-y_i} \\
&= \sum_{i=1}^m \left( y_i ln \frac{e^{-\omega^T x_i}}{1+e^{-\omega^T x_i}} + (1-y_i) ln \frac{1}{1+e^{-\omega^T x_i}} \right)
\end{aligned}</script><p>这里使用经典的方法，$l(\omega)$对$\omega$求导，闭式解不好表示，所以可以使用梯度上升的方法表示，其中</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{dl(\omega)}{d\omega} &= \sum_{i=1}^m \left( y_ix_i-\frac{x_i e^{-\omega^T x_i}}{1+e^{-\omega^T x_i}} \right) \\
\omega &= \omega+\frac{dl(\omega)}{d\omega}
\end{aligned}</script><p>将批量梯度下降转换成随机梯度下降有</p>
<script type="math/tex; mode=display">
\begin{aligned}
& for\quad i\; \in \; \{1,2,...,m\} \\
& \quad\quad\omega = \omega+\alpha \cdot \left(y_i-\frac{e^{-\omega^T x_i}}{1+e^{-\omega^T x_i}} \right) \cdot x_i
\end{aligned}</script><h5 id="Generalized-Linear-Models"><a href="#Generalized-Linear-Models" class="headerlink" title="Generalized Linear Models"></a>Generalized Linear Models</h5><p>上面提到，广义线性模型的形式如下&lt;/b&gt;<script type="math/tex">y=g(\omega^Tx+b)</script>联系线性回归、logistic回归。广义线性模型的训练，如果满足条件，其随机梯度下降的训练方法如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& for\quad i\; \in \; \{1,2,...,m\} \\
& \quad\quad\omega = \omega+\alpha \cdot \left(y_i-h_{\theta}(x_i) \right) \cdot x_i
\end{aligned}</script><p>其中，$h_{\theta}(x) = g(\theta^Tx)$即是目标模型的形式.</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2016/12/13/Logistic-Regression/" data-id="ckg3volwi0054p0phbq0rv7zr" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Expection-Maximization" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2016/12/09/Expection-Maximization/">Expection Maximization</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2016/12/09/Expection-Maximization/">
            <time datetime="2016-12-09T14:58:01.000Z" itemprop="datePublished">2016-12-09</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/machine-learning/">machine learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <h5 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h5><h6 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h6><p>从极大似然法的角度引入EM算法，先考虑这样的一个问题。</p>
<p>给定训练数据，假设训练数据满足高斯分布$f(x|\mu,\sigma^2)$，求这个分布的参数.</p>
<p>这便是典型的极大似然估计问题，对数似然函数为<script type="math/tex">l(\mu,\sigma^2|X)=\sum_iln(f(x_i|\mu,\sigma^2))</script>上面的式子分别对$\mu$和$\sigma$求偏导，并设置其偏导数为0，那么$\mu$和$\sigma$的解可以直接计算。</p>
<h6 id="包含隐含变量的极大似然估计"><a href="#包含隐含变量的极大似然估计" class="headerlink" title="包含隐含变量的极大似然估计"></a>包含隐含变量的极大似然估计</h6><p>假设还有一个条件：数据X有两个类别，$\lambda_1,\lambda_2$分别表示$f(x|\mu_1,\sigma_1^2)$和$f(x|\mu_2,\sigma_2^2)$在总体中的比率。因此总体分布就可以是$g(x|\lambda_1,\lambda_2,\mu_1,\mu_2,\sigma_1^2,\sigma_2^2)$，即两个分布的混合。所以有<script type="math/tex">g(x|\lambda_1,\lambda_2,\mu_1,\mu_2,\sigma_1^2,\sigma_2^2)=\lambda_1f(x|\mu_1,\sigma_1^2)+\lambda_2f(x|\mu_2,\sigma_2^2) \quad s.t.: \quad \lambda_1+\lambda_2=1</script>极大似然估计的求解方法如下，似然公式为<script type="math/tex">l()=log(P(X|\lambda_1,\lambda_2,\mu_1,\mu_2,\sigma_1^2,\sigma_2^2))=log\left(\sum_ig(x_i|\lambda_1,\lambda_2,\mu_1,\mu_2,\sigma_1^2,\sigma_2^2) \right)</script>上面式子是先求和在取对数，求偏导的方法就不行了！</p>
<h6 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h6><p>如果一个函数满足：对于任意的$x$都有$f^{‘’}(x) \ge 0$，那么$f(x)$是凸函数(如果输入$x$是向量，那么对应于其hessian矩阵(半)正定)，如果等号可以去掉即可称之为<strong>严格凸函数</strong>。<br>Jensen不等式的表述如下：假设$f$是凸函数，X是一个随机变量，那么有：<script type="math/tex">E[f(X)] \ge f(E[X])</script>对立面的表述是，如果函数$f$是凹(concave)函数($f^{‘’}(x) \le 0$)，那么有<script type="math/tex">E[f(X)] \le f(E[X])</script>当不等式中的等号成立时，$f(X)$是常数函数。</p>
<h6 id="期望最大化"><a href="#期望最大化" class="headerlink" title="期望最大化"></a>期望最大化</h6><p>现在抽象化问题，已知模型为$p(x|\theta)$，$X=(x_1,x_2,…,x_n)$，求$\theta$。这里需要引入隐含变量$Z=(z_1,z_2,…,z_m)$，所以有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(X|\theta)=\sum_zP(x_i|z,\theta) \cdot P(z|\theta)
\end{aligned}</script><p>定义似然函数为<script type="math/tex">l(\theta)=log(P(X|\theta))=log\sum_z(P(x_i|z,\theta) \cdot P(z|\theta))</script> EM算法也是通过迭代方法求得$l(\theta)$的极值，假设第$n$轮迭代计算出的$\theta$为$\theta_n$，只要下一轮的$\theta$优于$\theta_n$即可，推导如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
l(\theta)-l(\theta_n) &= log(P(X|\theta))-log(P(X|\theta_n)) \\
&= log(\sum_zP(x_i|z,\theta) \cdot P(z|\theta))-log(P(X|\theta_n)) \\
&= log(\sum_z P(z|X,\theta_n)\cdot \frac{P(x_i|z,\theta) \cdot P(z|\theta)}{P(z|X,\theta_n)})-log(P(X|\theta_n)) \\
&\ge \sum_z P(z|X,\theta_n) \cdot log\left( \frac{P(x_i|z,\theta) \cdot P(z|\theta)}{P(z|X,\theta_n)} \right) - \sum_z P(z|X,\theta_n) \cdot log(P(X|\theta_n)) \\
&= \sum_z P(z|X,\theta_n) \cdot log\left( \frac{P(x_i|z,\theta) \cdot P(z|\theta)}{P(z|X,\theta_n) \cdot P(X|\theta_n)} \right) \\
\end{aligned}</script><p>进而有</p>
<script type="math/tex; mode=display">
\begin{aligned}
l(\theta) &\ge l(\theta_n)+\sum_z P(z|X,\theta_n) \cdot log\left( \frac{P(x_i|z,\theta) \cdot P(z|\theta)}{P(z|X,\theta_n) \cdot P(X|\theta_n)} \right) \\
Set : l(\theta) &\ge M(\theta_n,\theta) \\
PS : l(\theta_n)&=M(\theta_n,\theta_n)
\end{aligned}</script><p>所以，我们只要</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{n+1} &= \mathop{arg\;max}_{\theta} M(\theta_n,\theta) \\
&= \mathop{arg\;max}_{\theta} \sum_z P(z|X,\theta_n) \cdot log(P(X|z,\theta) \cdot P(z|\theta)) \\
&= \mathop{arg\;max}_{\theta} \sum_z P(z|X,\theta_n) \cdot log(P(X,z|\theta)) \\
&= \mathop{arg\;max}_{\theta} E_{Z|X,\theta_n}[log(P(X,z|\theta)] \\
Set: F(\theta)&=E_{Z|X,\theta_n}[log(P(X,z|\theta)]
\end{aligned}</script><p>所以，EM算法的步骤如下：</p>
<hr>
<p>随机初始化$\theta_0$<br>迭代直到收敛：<br>E步：求条件期望$F(\theta,\theta_n)$<br>M步：求$F(\theta,\theta_n)$的极值$\theta_{n+1}$</p>
<hr>
<h5 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h5><p>高斯混合模型是EM算法在高斯分布上的应用。多元高斯分布函数的定义如下：<script type="math/tex">p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} e^{-\frac{(x-\mu)^T \Sigma^{-1} (x-\mu) }{2}}</script> 其中，$\mu,\Sigma$分别是均值和协方差矩阵。混合高斯模型的定义为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_M(x)&=\sum_{i=1}^k \alpha_i \cdot p(x|\mu_i,\Sigma_i) \\
s.t.: \sum_{i=1}^k \alpha_i &= 1
\end{aligned}</script><p>为了方便性，定义$j$个样本由第$i$个高斯分布产生的概率：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_M(z_j=i|x_j)&=\gamma_{ji} \\
&= \frac{P(z_j=i) \cdot p_M(x_j|z_j=i)}{p_M(x_j)} \\
&= \frac{\alpha_i \cdot p(x_j|\mu_i,\Sigma_i)}{\sum_{l=1}^k \alpha_l \cdotp(x_j|\mu_l,\Sigma_l)}
\end{aligned}</script><p>于是，给定样本集$D={x_1,…,x_m}$，用极大似然估计法，最大化似然对数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
l(D)=ln\left( \prod_{j=1}^m p_M(x_j) \right)=\sum_{j=1}^m ln\left( \sum_{i=1}^k \alpha_i \cdot p(x_j|\mu_i,\Sigma_i) \right)
\end{aligned}</script><p>上式中，分别由$l(D)$对$\mu_i,\Sigma_i,\alpha_i$求偏导，由于$p_M(z_j=i|x_j)=\gamma_{ji}$，所以令偏导数=0的结果如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mu_i &= \frac{\sum_{j=1}^m \gamma_{ji} \cdot x_j}{\sum_{j=1}^m \gamma_{ji}} \\
\Sigma_i &= \frac{\sum_{j=1}^m \gamma_{ji}(x_j-\mu_i)(x_j-\mu_i)^T}{\sum_{j=1}^m \gamma_{ji}} \\
\end{aligned}</script><p>对于混合系数$\alpha_i$，除了考虑$l(D)$外，他还有一个限制$\sum_{i=1}^k \alpha_i=1$，所以这里可以使用拉格朗日乘子法：<script type="math/tex">l(D)+\lambda \left( \sum_{i=1}^k \alpha_i-1 \right)</script>上式对$\alpha_i$求导并设结果为0有(注意联系$\gamma_{ji}的定义式$)</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{j=1}^m \frac{p(x_j|\mu_i,\Sigma_i)}{\sum_{l=1}^k \alpha_l \cdot p(x_j|\mu_l,\Sigma_l)} &= -\lambda \\
multiply\; \alpha_i,\; and\;get\; sum\; with\; respect\; to\; i \\
\sum_{i=1}^k \sum_{j=1}^m \frac{\alpha_i \cdot p(x_j|\mu_i,\Sigma_i)}{\sum_{l=1}^k \alpha_l \cdot p(x_j|\mu_l,\Sigma_l)} &= \sum_{i=1}^k -\lambda \cdot \alpha_i \\
hence: \lambda &= -m \\
\alpha_i &= \frac{1}{m} \sum_{j=1}^m \gamma_{ji}
\end{aligned}</script><p>所以，混合高斯的EM方法就是：先根据当前参数计算每个样本属于每个高斯成分的后验概率$\gamma_{ji}$（E步）；然后根据上面的规则更新模型参数${(\mu_i,\Sigma_i,\alpha_i)|1\le i \le k}$.</p>
<p>最后给出<b>高斯混合聚类算法</b>表述：</p>
<hr>
<p>输入：样本集$D={x_1,…,x_m}$，高斯混合成分个数$k$;</p>
<hr>
<p>1  初始化高斯混合分布的模型参数${(\mu_i,\Sigma_i,\alpha_i)|1\le i \le k}$.<br>2  迭代，直到满足停止条件<br>3  $\quad  for\; j=1,…m \quad do$<br>4  $\quad \quad$计算$p_M(z_j=i|x_j)=\gamma_{ji},(1 \le i \le k)$<br>5  $\quad end\; for$<br>6  $\quad  for\; i=1,…k \quad do$<br>7  $\quad \quad$更新模型参数$(\mu_i,\Sigma_i,\alpha_i)$<br>8  $C_i=\emptyset,(1\le i \le k)$<br>9  $for\; j=1,…m \quad do$<br>10 $\quad$根据最大后验概率规则确定每个$x_j$的簇$\lambda_j$<br>11 $\quad C_{\lambda_j}=C_{\lambda_j}\cup \{x_j\}$<br>12 $end\; for$</p>
<hr>
<p>输出：簇划分结果$C={C_1,..,C_k}$</p>
<hr>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2016/12/09/Expection-Maximization/" data-id="ckg3volvg002tp0phd5qw0fgy" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-DDoS的简单实现" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2016/12/06/DDoS的简单实现/">DDoS的简单实现</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2016/12/06/DDoS的简单实现/">
            <time datetime="2016-12-06T02:52:57.000Z" itemprop="datePublished">2016-12-06</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Dev/">Dev</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/other/">other</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <h5 id="Scapy实现SYN泛洪攻击"><a href="#Scapy实现SYN泛洪攻击" class="headerlink" title="Scapy实现SYN泛洪攻击"></a>Scapy实现SYN泛洪攻击</h5><center>![](http://ww4.sinaimg.cn/large/9bcfe727jw1fagvyplc77j20b0052mxg.jpg)</center>

<p>Scapy是一个可以让用户发送、侦听和解析并伪装网络报文的Python程序。这些功能可以用于制作侦测、扫描和攻击网络的工具。它的作用很多，简单如上图描述。</p>
<p>SYN泛洪攻击(SYN Flood)是一种比较常用的DoS方式之一。通过发送大量伪造的Tcp连接请求，使被攻击主机资源耗尽(通常是CPU满负荷或者内存不足) 的攻击方式。SYN泛洪攻击利用三次握手，客户端向服务器发送SYN报文之后就不再响应服务器回应的报文。由于服务器在处理TCP请求时，会在协议栈留一块缓冲区来存储握手的过程，当然如果超过一定的时间内没有接收到客户端的报文，本次连接在协议栈中存储的数据将会被丢弃。攻击者如果利用这段时间发送大量的连接请求，全部挂起在半连接状态。这样将不断消耗服务器资源，直到拒绝服务。</p>
<p>利用scapy构造一个SYN数据包的方法是：</p>
<pre><code>pkg = IP(src=&quot;202.121.0.12&quot;,dst=&quot;192.168.0.100&quot;)/TCP(sport=100,dport=80,flags=&quot;S&quot;)
send(pkt)
</code></pre><p>其中，IP包中指定了源地址src和目的地址dst，其中src是我们伪造的地址，这是DoS攻击中保护攻击者的一种方式。<br>flags的值我们设定为S,说明我们要发送的是SYN数据包，目标端口dport为80，发送端口sport为100。</p>
<h5 id="Socket实现DDoS攻击"><a href="#Socket实现DDoS攻击" class="headerlink" title="Socket实现DDoS攻击"></a>Socket实现DDoS攻击</h5><p>总体采用CS模式，客户机连接服务器，服务器发送指令，然后客户机发起攻击，客户机使用伪装的IP攻击。<br>事先规定攻击命令：</p>
<pre><code>#-H xxx.xxx.xxx.xxx -p xxxx -c &lt;start|stop&gt;
</code></pre><p>‘xxx.xxx.xxx.xxx’是目标地址， xxxx表示端口号，int型<br>命令可以是start：开始攻击；stop：停止攻击</p>
<h6 id="python中的socket使用"><a href="#python中的socket使用" class="headerlink" title="python中的socket使用"></a>python中的socket使用</h6><p><b>客户端</b><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line">#创建socket: AF_INET表示IPV4协议, SOCK_STREAM表示基于流的TCP协议</span><br><span class="line">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">#建立连接, 指定服务器的IP和端口</span><br><span class="line">s.connect((<span class="string">'192.168.0,100'</span>, <span class="number">7786</span>))</span><br></pre></td></tr></table></figure></p>
<p><b>服务器</b><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line">cliList = []</span><br><span class="line"># 创建socket</span><br><span class="line">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line"># 绑定地址和端口号</span><br><span class="line">s.bind((<span class="string">'0.0.0.0'</span>, <span class="number">7786</span>))：</span><br><span class="line"># 开始监听，指定最大连接数为10</span><br><span class="line">s.listen(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">while</span> True:</span><br><span class="line">    # 接受一个新的连接:</span><br><span class="line">    sock, addr = s.accept()</span><br><span class="line">    #将sock添加到列表中</span><br><span class="line">    cliList.append(sock)</span><br></pre></td></tr></table></figure></p>
<h6 id="python多线程-amp-多进程"><a href="#python多线程-amp-多进程" class="headerlink" title="python多线程 &amp; 多进程"></a>python多线程 &amp; 多进程</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t = Thread(target = func, args = (arg1, arg2))</span><br><span class="line">t.start()</span><br><span class="line"></span><br><span class="line">p = Process(target = func, args = (arg1, arg2))</span><br><span class="line">p.start()</span><br></pre></td></tr></table></figure>
<p>客户机接受了服务器的命令后，启动一个进程发动攻击，一个客户端可以伪造不同的IP发送大量的SYN请求，大量客户端一起工作瘫痪目标。</p>
<p><a href="https://github.com/Atlantic8/Project/tree/master/simple%20implementation%20of%20DDoS" target="_blank" rel="noopener">具体的实现点击这里</a></p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2016/12/06/DDoS的简单实现/" data-id="ckg3volv50027p0phcgmxcmos" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Floyd-Cycle-Detection" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2016/12/01/Floyd-Cycle-Detection/">Floyd Cycle Detection</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/2016/12/01/Floyd-Cycle-Detection/">
            <time datetime="2016-12-01T01:06:54.000Z" itemprop="datePublished">2016-12-01</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
    </div>

                        
                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <h6 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h6><p>Floyd判圈算法(Floyd Cycle Detection Algorithm)，又称龟兔赛跑算法(Tortoise and Hare Algorithm)。该算法由美国科学家罗伯特·弗洛伊德发明，是一个可以在有限状态机、迭代函数或者链表上判断是否存在环，求出该环的起点与长度的算法。</p>
<pre><code>如果有限状态机、迭代函数或者链表上存在环，那么在某个环上以不同速度前进的2个指针必定会在某个时刻相遇
如果从同一个起点(即使这个起点不在某个环上)同时开始以不同速度前进的2个指针最终相遇，那么可以判定存在一个环，且可以求出2者相遇处所在的环的起点与长度
</code></pre><h6 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h6><pre><code>假设已知某个起点节点为节点S。现设两个指针t和h，将它们均指向S
同时让t和h往前推进，但是二者的速度不同：t每前进1步，h前进2步
    当h无法前进，即到达某个没有后继的节点时，就可以确定从S出发不会遇到环
    当t与h再次相遇(在点M)时，就可以确定从S出发一定会进入某个环，设其为环C
        令h仍均位于节点M，而令t返回起点节点S
        让t和h往前推进，且保持二者的速度都为1
        t和h相遇的地方即为环C的入口
</code></pre><h6 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h6><p><a href="http://atlantic8.github.io/2016/09/04/Linked-List-Cycle/">Linked List Cycle</a></p>
<p><a href="http://atlantic8.github.io/2016/12/01/Happy-Number/">Happy Number</a></p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="atlantic8.github.io/2016/12/01/Floyd-Cycle-Detection/" data-id="ckg3volvn003ep0phkfwgwuv5" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/page/5/">&laquo; 上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/7/">下一页 &raquo;</a>
    </nav>
</section>
            
                
<aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title">最新文章</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/09/01/Python-Functions/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Dev/">Dev</a></p>
                            <p class="item-title"><a href="/2020/09/01/Python-Functions/" class="title">Python Functions</a></p>
                            <p class="item-date"><time datetime="2020-09-01T15:15:14.000Z" itemprop="datePublished">2020-09-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/09/01/Bounds-in-Binary-Search/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/OJ/">OJ</a></p>
                            <p class="item-title"><a href="/2020/09/01/Bounds-in-Binary-Search/" class="title">Bounds in Binary Search</a></p>
                            <p class="item-date"><time datetime="2020-09-01T15:12:12.000Z" itemprop="datePublished">2020-09-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/09/01/BM25/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Algorithm/">Algorithm</a></p>
                            <p class="item-title"><a href="/2020/09/01/BM25/" class="title">BM25</a></p>
                            <p class="item-date"><time datetime="2020-09-01T15:06:49.000Z" itemprop="datePublished">2020-09-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/09/01/Similarity-and-Relativity-Metrics/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Math/">Math</a></p>
                            <p class="item-title"><a href="/2020/09/01/Similarity-and-Relativity-Metrics/" class="title">Similarity and Relativity Metrics</a></p>
                            <p class="item-date"><time datetime="2020-09-01T15:04:31.000Z" itemprop="datePublished">2020-09-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2020/09/01/Entropy-and-Related-Metrics/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Math/">Math</a></p>
                            <p class="item-title"><a href="/2020/09/01/Entropy-and-Related-Metrics/" class="title">Entropy and Related Metrics</a></p>
                            <p class="item-date"><time datetime="2020-09-01T15:01:41.000Z" itemprop="datePublished">2020-09-01</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">50</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dev/">Dev</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/OJ/">OJ</a><span class="category-list-count">56</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">十二月 2016</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a><span class="archive-list-count">35</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">七月 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">六月 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">五月 2016</a><span class="archive-list-count">8</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/Backtracking/" style="font-size: 11.11px;">Backtracking</a> <a href="/tags/Binary-Search/" style="font-size: 12.22px;">Binary Search</a> <a href="/tags/Binary-Tree/" style="font-size: 16.67px;">Binary Tree</a> <a href="/tags/Cpp/" style="font-size: 15.56px;">Cpp</a> <a href="/tags/DFS/" style="font-size: 10px;">DFS</a> <a href="/tags/DP/" style="font-size: 16.67px;">DP</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Divide-Conquer/" style="font-size: 10px;">Divide & Conquer</a> <a href="/tags/Game-Theory/" style="font-size: 10px;">Game Theory</a> <a href="/tags/Geometry/" style="font-size: 10px;">Geometry</a> <a href="/tags/Graph/" style="font-size: 11.11px;">Graph</a> <a href="/tags/Greedy/" style="font-size: 13.33px;">Greedy</a> <a href="/tags/IPython/" style="font-size: 10px;">IPython</a> <a href="/tags/Java/" style="font-size: 13.33px;">Java</a> <a href="/tags/LeetCode/" style="font-size: 20px;">LeetCode</a> <a href="/tags/Leetcode/" style="font-size: 11.11px;">Leetcode</a> <a href="/tags/MIR/" style="font-size: 10px;">MIR</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/Math/" style="font-size: 12.22px;">Math</a> <a href="/tags/Matlab/" style="font-size: 10px;">Matlab</a> <a href="/tags/NLP/" style="font-size: 14.44px;">NLP</a> <a href="/tags/Optimization/" style="font-size: 10px;">Optimization</a> <a href="/tags/POJ/" style="font-size: 11.11px;">POJ</a> <a href="/tags/Permutation/" style="font-size: 10px;">Permutation</a> <a href="/tags/STL/" style="font-size: 10px;">STL</a> <a href="/tags/Sliding-window/" style="font-size: 14.44px;">Sliding window</a> <a href="/tags/Sort/" style="font-size: 11.11px;">Sort</a> <a href="/tags/State-Machine/" style="font-size: 10px;">State Machine</a> <a href="/tags/String/" style="font-size: 14.44px;">String</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/bit/" style="font-size: 10px;">bit</a> <a href="/tags/deep-learning/" style="font-size: 15.56px;">deep learning</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/machine-learning/" style="font-size: 18.89px;">machine learning</a> <a href="/tags/music-information-retrieval/" style="font-size: 10px;">music information retrieval</a> <a href="/tags/numpy/" style="font-size: 11.11px;">numpy</a> <a href="/tags/other/" style="font-size: 10px;">other</a> <a href="/tags/pandas/" style="font-size: 11.11px;">pandas</a> <a href="/tags/prime/" style="font-size: 10px;">prime</a> <a href="/tags/python/" style="font-size: 17.78px;">python</a> <a href="/tags/random-algorithm/" style="font-size: 12.22px;">random algorithm</a> <a href="/tags/recommender-system/" style="font-size: 10px;">recommender system</a> <a href="/tags/search/" style="font-size: 10px;">search</a> <a href="/tags/time-series-data/" style="font-size: 10px;">time_series_data</a> <a href="/tags/visualization/" style="font-size: 10px;">visualization</a> <a href="/tags/web/" style="font-size: 10px;">web</a> <a href="/tags/数据分析/" style="font-size: 10px;">数据分析</a> <a href="/tags/文件/" style="font-size: 10px;">文件</a>
        </div>
    </div>

    
        
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">链接</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="https://xueshu.glgoo.org/">Google Scholar Mirror</a>
                    </li>
                
                    <li>
                        <a href="https://www.kaggle.com/">Kaggle</a>
                    </li>
                
                    <li>
                        <a href="http://mlr.cs.umass.edu/ml/datasets.html">UCI dataset</a>
                    </li>
                
                    <li>
                        <a href="https://leetcode.com/problemset/algorithms/">LeetCode</a>
                    </li>
                
            </ul>
        </div>
    </div>


    
    <div id="toTop" class="fas fa-angle-up"></div>
</aside>

            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2020 曹文强<br>
            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        </div>
    </div>
</footer>
        


    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>